# Embodied-AI-papers
[![Awesome](https://awesome.re/badge.svg)](https://github.com/zjukg/KG-LLM-Papers) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/zjukg/KG-LLM-Papers/blob/main/LICENSE)
![](https://img.shields.io/github/last-commit/zjukg/KG-LLM-Papers?color=green) 
![](https://img.shields.io/badge/PRs-Welcome-red) 

>What can LLMs do for ROBOTs? 

ðŸ™Œ This repository collects papers integrating **Embodied AI** and **large language models (LLMs)**.

ðŸ˜Ž Welcome to recommend missing papers through **`Adding Issues`** or **`Pull Requests`**. 

## ðŸ“œ Content

-[Embodied-AI-papers](#embodied-ai-papers)
  - [ðŸ¤–ðŸŒ„ KG-driven Multi-modal Learning (KG4MM)](#-kg-driven-multi-modal-learning-kg4mm)
- \[[AAAI 2024](https://ojs.aaai.org/index.php/AAAI/article/view/29828)\] Beyond Entities: A Large-Scale Multi-Modal Knowledge Graph with Triplet Fact Grounding.