# Embodied-AI-papers
[![Awesome](https://awesome.re/badge.svg)](https://github.com/zjukg/KG-LLM-Papers) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/zjukg/KG-LLM-Papers/blob/main/LICENSE)
![](https://img.shields.io/github/last-commit/zjukg/KG-LLM-Papers?color=green) 
![](https://img.shields.io/badge/PRs-Welcome-red) 

>What can LLMs do for ROBOTs? 

ðŸ™Œ This repository collects papers integrating **Embodied AI** and **large language models (LLMs)**.

ðŸ˜Ž Welcome to recommend missing papers through **`Adding Issues`** or **`Pull Requests`**. 

## ðŸ“œ Content

- [Embodied-AI-papers](#embodied-ai-papers)
  - [ðŸ“œContent](#-content)
    - [ðŸ¤–ðŸŒ„ Grounding Language to Robot Skills](#-grounding-language-to-robot-skills)
   
### Grounding Language to Robot Skills

- \[[arXiv 2024](https://arxiv.org/abs/2210.01911)\] Grounding Language with Visual Affordances over Unstructured Data.
