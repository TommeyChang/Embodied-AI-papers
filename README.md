# Embodied-AI-papers
[![Awesome](https://awesome.re/badge.svg)](https://github.com/zjukg/KG-LLM-Papers) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/zjukg/KG-LLM-Papers/blob/main/LICENSE)
![](https://img.shields.io/github/last-commit/zjukg/KG-LLM-Papers?color=green) 
![](https://img.shields.io/badge/PRs-Welcome-red) 

>What can LLMs do for ROBOTs? 

ðŸ™Œ This repository collects papers integrating **Embodied AI** and **large language models (LLMs)**.

ðŸ˜Ž Welcome to recommend missing papers through **`Adding Issues`** or **`Pull Requests`**. 

ðŸ¥½ The papers are ranked according to our **subjective** opinions.

## ðŸ“œ Table of Content

- [Embodied-AI-papers](#embodied-ai-papers)
  - [ðŸ“œ Table of Content](#-table-of-content)
  - [Highlights](#highlights)
  - [Collected](#collected)
    - [Survey](#survey)
    - [Datasets](#datasets)
    - [General Tasks](#general-tasks)
    - [Language \& Manipulation](#language--manipulation)

## Highlights


## Collected

### Survey

- \[[arXiv](https://arxiv.org/pdf/2402.05741)\] Real-World Robot Applications of Foundation Models: A Review

### Datasets

- \[[CoRL 2023 Workshop TGR](https://openreview.net/forum?id=zraBtFgxT0&noteId=kNJ60a3jR5)\] Open X-Embodiment: Robotic Learning Datasets and RT-X Models \[[Project](https://robotics-transformer-x.github.io)\] \[[Code](https://github.com/google-deepmind/open_x_embodiment)\]

### General Tasks

- \[[ICRA 2024 Workshop VLMNM](https://openreview.net/forum?id=jGrtIvJBpS)\] Octo: An Open-Source Generalist Robot Policy
- \[[arXiv](https://arxiv.org/pdf/2403.12761)\] BTGenBot: Behavior Tree Generation for Robotic Tasks with Lightweight LLMs

### Language \& Manipulation

- \[[arXiv 2024](https://arxiv.org/abs/2403.17124)\] Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
- \[[arXiv 2024](https://arxiv.org/abs/2305.19075)\] Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data
- \[[arXiv 2023](https://arxiv.org/abs/2210.01911)\] Grounding Language with Visual Affordances over Unstructured Data